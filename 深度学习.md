## 卷积的作用
- 卷积的主要作用是提取特征，比如拉普拉斯算子就是一个卷积核，通过二阶微分检测边缘与原图像叠加后达到锐化的效果。
- 与sobel、Robert等一阶算子比较
1. 一阶导数产生较粗的边缘。
2. 二阶导数对细线、噪声、孤立点有较强响应。
3. 二阶导数的符号可以确定边缘的过度是从暗到亮还是从亮到暗。
- sobel、prewitt、roberts算子比较
1. roberts算子边缘定位准，但是对噪声敏感。适用于边缘明显且噪声较少的图像分割。
2. prewitt算子通过像素平均抑制噪声，相当于低通滤波，会造成高频信息丢失，使图像模糊。对边缘的定位不如Roberts算子
3. sobel算子认为邻域像素对当前像素的影响使不等价的，距离越近，影响越大，做加权平均。
## 卷积核为什么为奇数
- 更容易padding（填充0）
1. 若卷积时要求图像卷积前后尺寸不变，需要padding。假设图像大小为 n*n，卷积核为k*k。步长为1，padding为（k - 1）/2时，卷积输出为(n-k+2*((k-1)/2))/1+1=n，保证卷积前后尺寸不变。若k为偶数，（k - 1）/ 2非整数。
2. 更容易找到卷积锚点。进行卷积操作一般以某个位置为基准进行滑动，通常为卷积核中心。若卷积核为偶数，不好确定锚点。
## 为什么用小卷积核代替大卷积
- 用小卷积代替大卷积的前提是保证两者具有相同大小的输出和感受野。当stride为1，padding为0。1个5乘5卷积可用2个3乘3卷积代替。（n - 5）/ 1 + 1 = n - 4 而（n - 3）/ 1 + 1 = n - 2
（n - 2 - 3）/ 1 + 1 = n - 4
1. 多个小卷积核叠加可以增加非线性表达能力。
2. 可以减少参数，降低计算量。
## 卷积神经网络
### 卷积层
- 特点
1. 拥有局部感知特性
2. 权值共享
3. 卷积核深度与输入特征层深度相同
4. 输出特征矩阵的深度与卷积核个数相同
- 感受野
1. 输出特征图上的一个单元对应输入层上的区域大小
### 池化层
- 特点
1. 没有训练参数
2. 只改变特征矩阵的高和宽，不改变深度
3. 一般池化核大小与步长相同
- 作用
1. 增大感受野
2. 实现特征降维，减少参数，降低计算量
3. 增加不变性（平移不变性、旋转不变性、尺度不变性）


- 激活函数
1. Sigmoid激活函数：饱和时梯度非常小，网络层数较深易出现梯度消失。
2. Relu激活函数：反向传播过程有非常大的梯度经过时，可能导致权重分布中心小于0，反向传播无法更新权重，即进入失活状态。

## 深度网络
- 好处
1. 特征的“等级”随网络深度的加深而变高
2. 深层网络有更好的表达能力
- 问题

1. 梯度弥散、梯度爆炸
   1. 反向传播计算梯度优化的神经网络
# 网络结构
## MobileNetV1
- 介绍

专注于移动端或者嵌入式设备中的轻量级CNN网络。相比传统神经网络小幅度降低准确率，但大大减少模型参数和运算量。（相比VGG16准确率降低0.9%，参数只有VGG的1/32）
- 网络亮点
1. DW卷积大大减少运算量和参数
2. 增加超参数alpha和beta alpha用来控制卷积核的个数 beta为调整图像分辨率的参数。
- 与传统网络对比
1. 传统卷积
   1. 卷积核核channel = 输入特征矩阵channel
   2. 输出特征矩阵channel = 卷积核个数
2. DW卷积
   1. 卷积核channel = 1
   2. 输入特征矩阵channel = 卷积核个数 = 输出特征矩阵channel
3. 深度可分离卷积 DW卷积 + PW卷积
   1. pw卷积为普通1乘1卷积，channel为输入特征矩阵channel。
## MobileNetV2
- 解决了V1网络dw部分卷积核废掉，即卷积核参数大部分为0.
- 网络亮点
1. Inverted Residuals（倒残差结构）
   1. 1乘1卷积升维
   2. 3乘3卷积DW
   3. 1乘1卷积降维
   4. ReLu6激活函数 输入小于0置0，大于6置6
2. Linear Bottlenecks
   1. 倒残差结构的最后1乘1卷积层使用线性激活函数，原因是ReLU激活函数对低维特征信息造成大量损失。