## 卷积的作用
- 卷积的主要作用是提取特征，比如拉普拉斯算子就是一个卷积核，通过二阶微分检测边缘与原图像叠加后达到锐化的效果。
- 与sobel、Robert等一阶算子比较
1. 一阶导数产生较粗的边缘。
2. 二阶导数对细线、噪声、孤立点有较强响应。
3. 二阶导数的符号可以确定边缘的过度是从暗到亮还是从亮到暗。
- sobel、prewitt、roberts算子比较
1. roberts算子边缘定位准，但是对噪声敏感。适用于边缘明显且噪声较少的图像分割。
2. prewitt算子通过像素平均抑制噪声，相当于低通滤波，会造成高频信息丢失，使图像模糊。对边缘的定位不如Roberts算子
3. sobel算子认为邻域像素对当前像素的影响使不等价的，距离越近，影响越大，做加权平均。
## 卷积核为什么为奇数
- 更容易padding（填充0）
1. 若卷积时要求图像卷积前后尺寸不变，需要padding。假设图像大小为 n*n，卷积核为k*k。步长为1，padding为（k - 1）/2时，卷积输出为(n-k+2*((k-1)/2))/1+1=n，保证卷积前后尺寸不变。若k为偶数，（k - 1）/ 2非整数。
2. 更容易找到卷积锚点。进行卷积操作一般以某个位置为基准进行滑动，通常为卷积核中心。若卷积核为偶数，不好确定锚点。
## 为什么用小卷积核代替大卷积
- 用小卷积代替大卷积的前提是保证两者具有相同大小的输出和感受野。当stride为1，padding为0。1个5乘5卷积可用2个3乘3卷积代替。（n - 5）/ 1 + 1 = n - 4 而（n - 3）/ 1 + 1 = n - 2
（n - 2 - 3）/ 1 + 1 = n - 4
1. 多个小卷积核叠加可以增加非线性表达能力。
2. 可以减少参数，降低计算量。
## 池化的作用
- 增大感受野
- 实现特征降维，减少参数，降低计算量
- 增加不变性（平移不变性、旋转不变性、尺度不变性）

## 卷积神经网络
- 激活函数
1. Sigmoid激活函数：饱和时梯度非常小，网络层数较深易出现梯度消失。
2. Relu激活函数：反向传播过程有非常大的梯度经过时，可能导致权重分布中心小于0，反向传播无法更新权重，即进入失活状态。

## 深度网络
- 好处
1. 特征的“等级”随网络深度的加深而变高
2. 深层网络有更好的表达能力
- 问题

1. 梯度弥散、梯度爆炸
   1. 反向传播计算梯度优化的神经网络