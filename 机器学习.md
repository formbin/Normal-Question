## 深度学习
### 【译】理解LSTM（通俗易懂版）
- https://www.jianshu.com/p/95d5c461924c
### 详解机器学习中的梯度消失、爆炸原因及其解决方法
- https://blog.csdn.net/qq_25737169/article/details/78847691
- 梯度下降全局最优与局部最优

当损失函数为凸函数，梯度下降法得到的解一定是全局最优解。凸函数是指连接函数两点的线段，区间内函数图像在线段的下方。
- 梯度下降不收敛：
```
是有可能的。比如我们有梯度下降求解f(x)=x4的最小值。

假设初始点是x=2，学习率（步长）是0.5
初始的时候

x=2, f′(x)=32, f(x)=16
经过一次迭代

x=−14.0, f′(x)=−10976, f(x)=38416
又一次迭代

x=5474, f′(x)=656106545696, f(x)=897881807784976
我们看到x在来回摆荡，而且离最小值越来越远，显然这个情况下就
是因为学习率太大了，导致每次更新时都“过犹不及”“矫枉过正”，
所以最后并没有收敛。



如果学习率是0.1，

x=2, f′(x)=32, f(x)=16
x=−1.2, f′(x)=−6.91, f(x)=2.07
x=−0.51, f′(x)=−0.53, f(x)=0.06
x=−0.46, f′(x)=−0.38, f(x)=0.04
x=−0.42, f′(x)=−0.29, f(x)=0.03
x=−0.39, f′(x)=−0.24, f(x)=0.02
我们就看到是在逐渐收敛的了
```
### 深入理解Batch Normalization批标准化
- https://www.cnblogs.com/guoyaohua/p/8724433.html