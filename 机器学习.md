## 深度学习
### 【译】理解LSTM（通俗易懂版）
https://www.jianshu.com/p/95d5c461924c
### 详解机器学习中的梯度消失、爆炸原因及其解决方法
https://blog.csdn.net/qq_25737169/article/details/78847691
- 梯度下降

1. 作用：在机器学习算法中，当最小化损失函数时，可用梯度下降法求解，得到模型参数。
2. 原理：
- 梯度下降全局最优与局部最优
当损失函数为凸函数，梯度下降法得到的解一定是全局最优解。凸函数是指连接函数两点的线段，区间内函数图像在线段的下方。
- 梯度下降不收敛：
```
是有可能的。比如我们有梯度下降求解f(x)=x4的最小值。

假设初始点是x=2，学习率（步长）是0.5
初始的时候

x=2, f′(x)=32, f(x)=16
经过一次迭代

x=−14.0, f′(x)=−10976, f(x)=38416
又一次迭代

x=5474, f′(x)=656106545696, f(x)=897881807784976
我们看到x在来回摆荡，而且离最小值越来越远，显然这个情况下就
是因为学习率太大了，导致每次更新时都“过犹不及”“矫枉过正”，
所以最后并没有收敛。



如果学习率是0.1，

x=2, f′(x)=32, f(x)=16
x=−1.2, f′(x)=−6.91, f(x)=2.07
x=−0.51, f′(x)=−0.53, f(x)=0.06
x=−0.46, f′(x)=−0.38, f(x)=0.04
x=−0.42, f′(x)=−0.29, f(x)=0.03
x=−0.39, f′(x)=−0.24, f(x)=0.02
我们就看到是在逐渐收敛的了
```
### 深入理解Batch Normalization批标准化
- https://www.cnblogs.com/guoyaohua/p/8724433.html

### RF、GBDT、XGBoost面试级整理
- https://blog.csdn.net/qq_28031525/article/details/70207918
- RF
随机森林是一种bagging集成学习方法，由多个决策树组成。随机的意思是随机选择样本和特征。
- 随机森林的生成步骤
1. 在样本中通过重采样产生n个样本
2. 在n个样本中选择m个特征，用建立决策树的方式获得最佳分割点
3. 重复k次，产生k个决策树。
4. 对分类问题采用简单投票，对回归问题采用简单平均。
- GBDT
1. 是一种迭代的决策树算法，由多棵决策树组成，每棵树学习的是之前所有树的结论和残差。
2. 是回归树，在分支时以最小化平方误差为标准。
3. 可以发现有效的特征和特征组合。

- xgboost处理缺失值方法

在寻找分裂点时，只对该特征值的non-missing样本进行统计，减少了时间开销。将特征值为missing的样本分配到左节点和右节点，选择增益大的方向进行分裂。也可为缺失值指定默认方向，提升效率。若训练中没有缺失值而预测中出现缺失，会自动将缺失值放到右子树。

### 随机森林（RF）、GBDT、Xgboosta原理、优缺点、区别等面试要点总结
- https://blog.csdn.net/qq_25886325/article/details/93658975?utm_medium=distribute.pc_relevant.none-task-blog-title-8&spm=1001.2101.3001.4242

### 树模型集成学习(Tree Embedding)
- https://www.kesci.com/home/project/5d5bb8948499bc002c04c2b4
- bagging、boosting、stacking：
bagging由多个基分类器组成，每个分类器模型是相同的，每个基分类器选择部分样本，可以减少方差。投票得出结果。
boosting由多个串行分类器组成，每个分类器学习的是之前所有分类器的预测结果和真实值的差异。可以减少偏差
stacking由多个基分类器组成，每个分类器模型是不同的，每个基分类器用全部样本训练。再训练一个分类器，从各个预测中生成最后预测。


- 奥卡姆剃刀：简单的是最好的。
- 没有免费的午餐定理：没有一种机器学习算法是适用于所有情况的。

### Python 直接赋值、浅拷贝和深度拷贝解析
- https://www.runoob.com/w3cnote/python-understanding-dict-copy-shallow-or-deep.html

### SVM和LR区别
- 相同点：
1. 都是有监督的分类算法。
2. 不考虑核函数，都是线性分类器。（决策面都是线性的）
3. 都是判别模型，不考虑数据是怎么生成的，只关心数据之间的差别。生成模型先计算联合概率分布，再通过贝叶斯公式转化为条件概率。
- 不同点：
1. 损失函数不同，SVM为合页损失函数，LR为交叉熵损失函数。两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。
2. 影响SVM决策面的只有少数的支持向量，需要做归一化处理。影响LR决策面的是所有样本点，决策面倾向于远离样本点比较多的类，若类不平衡，需先做平衡处理。
3. SVM是结构风险最小化，在训练误差和模型复杂度之间寻求平衡，防止过拟合，减小泛化误差，因为损失函数中带有L2正则项。LR是经验风险最小化，只考虑误差最小
4. 在解决非线性问题，SVM可以采用核函数机制，LR不能。因为SVM只有少数样本参与核计算，LR每个样本都要参与核计算，计算复杂度高。
5. LR可以给出每个点属于每一类的概率，SVM不能。
6. SVM适用于小数据集，且效果比LR好。LR计算简单，更适合大数据集，可以在线训练。
- 场景选择
1. 特征数量较大，和样本数差不多，选用LR或者带线性核的SVM。
2. 特征数量较小，样本数量一般，选用带高斯核函数的SVM。
3. 特征数量较小，样本很多，手动添加特征变成第一种情况。
- SVM核函数选择
1. 如果特征维数很高，往往线性可分（SVM解决非线性分类问题的思路就是将样本映射到更高维的特征空间中），可以采用LR或者线性核的SVM；
2. 如果样本数量很多，由于求解最优化问题的时候，目标函数涉及两两样本计算内积，使用高斯核明显计算量会大于线性核，所以手动添加一些特征，使得线性可分，然后可以用LR或者线性核的SVM；
3. 如果不满足上述两点，即特征维数少，样本数量正常，可以使用高斯核的SVM。