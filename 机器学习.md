## 深度学习
### 【译】理解LSTM（通俗易懂版）
https://www.jianshu.com/p/95d5c461924c
### 详解机器学习中的梯度消失、爆炸原因及其解决方法
https://blog.csdn.net/qq_25737169/article/details/78847691
- 梯度下降全局最优与局部最优

当损失函数为凸函数，梯度下降法得到的解一定是全局最优解。凸函数是指连接函数两点的线段，区间内函数图像在线段的下方。
- 梯度下降不收敛：
```
是有可能的。比如我们有梯度下降求解f(x)=x4的最小值。

假设初始点是x=2，学习率（步长）是0.5
初始的时候

x=2, f′(x)=32, f(x)=16
经过一次迭代

x=−14.0, f′(x)=−10976, f(x)=38416
又一次迭代

x=5474, f′(x)=656106545696, f(x)=897881807784976
我们看到x在来回摆荡，而且离最小值越来越远，显然这个情况下就
是因为学习率太大了，导致每次更新时都“过犹不及”“矫枉过正”，
所以最后并没有收敛。



如果学习率是0.1，

x=2, f′(x)=32, f(x)=16
x=−1.2, f′(x)=−6.91, f(x)=2.07
x=−0.51, f′(x)=−0.53, f(x)=0.06
x=−0.46, f′(x)=−0.38, f(x)=0.04
x=−0.42, f′(x)=−0.29, f(x)=0.03
x=−0.39, f′(x)=−0.24, f(x)=0.02
我们就看到是在逐渐收敛的了
```
### 深入理解Batch Normalization批标准化
- https://www.cnblogs.com/guoyaohua/p/8724433.html

### RF、GBDT、XGBoost面试级整理
- https://blog.csdn.net/qq_28031525/article/details/70207918

- xgboost处理缺失值方法
在寻找分裂点时，只对该特征值的non-missing样本进行统计，减少了时间开销。将特征值为missing的样本分配到左节点和右节点，选择增益大的方向进行分裂。也可为缺失值指定默认方向，提升效率。若训练中没有缺失值而预测中出现缺失，会自动将缺失值放到右子树。

### 随机森林（RF）、GBDT、Xgboosta原理、优缺点、区别等面试要点总结
- https://blog.csdn.net/qq_25886325/article/details/93658975?utm_medium=distribute.pc_relevant.none-task-blog-title-8&spm=1001.2101.3001.4242

### 树模型集成学习(Tree Embedding)
- https://www.kesci.com/home/project/5d5bb8948499bc002c04c2b4