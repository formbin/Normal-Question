## 深度学习
### 【译】理解LSTM（通俗易懂版）
https://www.jianshu.com/p/95d5c461924c
### 详解机器学习中的梯度消失、爆炸原因及其解决方法
https://blog.csdn.net/qq_25737169/article/details/78847691
## 梯度下降
1. 作用：在机器学习算法中，当最小化损失函数时，可用梯度下降法求解，得到模型参数。
2. 原理：
- 梯度下降全局最优与局部最优
当损失函数为凸函数，梯度下降法得到的解一定是全局最优解。凸函数是指连接函数两点的线段，区间内函数图像在线段的下方。
- 梯度下降不收敛：
```
是有可能的。比如我们有梯度下降求解f(x)=x4的最小值。

假设初始点是x=2，学习率（步长）是0.5
初始的时候

x=2, f′(x)=32, f(x)=16
经过一次迭代

x=−14.0, f′(x)=−10976, f(x)=38416
又一次迭代

x=5474, f′(x)=656106545696, f(x)=897881807784976
我们看到x在来回摆荡，而且离最小值越来越远，显然这个情况下就
是因为学习率太大了，导致每次更新时都“过犹不及”“矫枉过正”，
所以最后并没有收敛。



如果学习率是0.1，

x=2, f′(x)=32, f(x)=16
x=−1.2, f′(x)=−6.91, f(x)=2.07
x=−0.51, f′(x)=−0.53, f(x)=0.06
x=−0.46, f′(x)=−0.38, f(x)=0.04
x=−0.42, f′(x)=−0.29, f(x)=0.03
x=−0.39, f′(x)=−0.24, f(x)=0.02
我们就看到是在逐渐收敛的了
```
- 批梯度下降
1. 能够获得全局最优解，但是更新每个参数需要遍历所有数据，计算量大。
- 随机梯度下降
1. 均匀随机选取一个样本乘N代表整体样本。优点是会跳到新的和潜在更好的局部最优解，缺点是
### 深入理解Batch Normalization批标准化
- https://www.cnblogs.com/guoyaohua/p/8724433.html

### RF、GBDT、XGBoost面试级整理
- https://blog.csdn.net/qq_28031525/article/details/70207918
- RF
随机森林是一种bagging集成学习方法，由多个决策树组成。随机的意思是随机选择样本和特征。
- 随机森林的生成步骤
1. 在样本中通过重采样产生n个样本
2. 在n个样本中选择m个特征，用建立决策树的方式获得最佳分割点
3. 重复k次，产生k个决策树。
4. 对分类问题采用简单投票，对回归问题采用简单平均。
- GBDT
1. 是一种迭代的决策树算法，由多棵决策树组成，每棵树学习的是之前所有树的结论和残差。
2. 是回归树，在分支时以最小化平方误差为标准。
3. 可以发现有效的特征和特征组合。

- xgboost处理缺失值方法

在寻找分裂点时，只对该特征值的non-missing样本进行统计，减少了时间开销。将特征值为missing的样本分配到左节点和右节点，选择增益大的方向进行分裂。也可为缺失值指定默认方向，提升效率。若训练中没有缺失值而预测中出现缺失，会自动将缺失值放到右子树。

### 随机森林（RF）、GBDT、Xgboosta原理、优缺点、区别等面试要点总结
- https://blog.csdn.net/qq_25886325/article/details/93658975?utm_medium=distribute.pc_relevant.none-task-blog-title-8&spm=1001.2101.3001.4242

### 树模型集成学习(Tree Embedding)
- https://www.kesci.com/home/project/5d5bb8948499bc002c04c2b4
- bagging、boosting、stacking：
bagging由多个基分类器组成，每个分类器模型是相同的，每个基分类器选择部分样本，可以减少方差。投票得出结果。
boosting由多个串行分类器组成，每个分类器学习的是之前所有分类器的预测结果和真实值的差异。可以减少偏差
stacking由多个基分类器组成，每个分类器模型是不同的，每个基分类器用全部样本训练。再训练一个分类器，从各个预测中生成最后预测。


- 奥卡姆剃刀：简单的是最好的。
- 没有免费的午餐定理：没有一种机器学习算法是适用于所有情况的。

### Python 直接赋值、浅拷贝和深度拷贝解析
- https://www.runoob.com/w3cnote/python-understanding-dict-copy-shallow-or-deep.html

## LR
### 对LR理解
- LR假设数据服从伯努利分布，利用极大化似然函数的方法，通过梯度下降法求解参数，达到二分类目的。
- 损失函数：交叉熵损失函数
- 为什么将特征进行离散化
1. 离散特征的增加和减少较容易，便于模型的快速迭代。
2. 稀疏向量内积乘法运算速度更快。
3. 对异常值更具鲁棒性
4. 模型更加稳定，某个变量在区间内变化不会对模型产生影响
5. LR为线性模型，离散化后每个变量有单独的权重，相当于引入了非线性，提升了模型的表达能力。
6. 离散化后可进行特征交叉，进一步引入非线性，提升表达能力。
- 为什么不用MSE作为损失函数
- https://www.jianshu.com/p/af1e5cff21b9
1. s = wx + b 且 h = 1/1 + e**-s 且 L(w) = (y - h)h(1 - h)x 当 h 为0或1会有梯度消失现象。
2. MSE的导数为非凸函数，求最优解困难。

### L1，L2正则等
- 区别
1. 绝对值的导数不连续，l2计算更方便
2. l1能够生稀疏权值矩阵，把不重要的特征置0，可用于特征选择。
3. L2有唯一解，L1不是。
- https://bbs.cvmart.net/topics/1222/vote_count?
### SVM和LR区别
- 相同点：
1. 都是有监督的分类算法。
2. 不考虑核函数，都是线性分类器。（决策面都是线性的）
3. 都是判别模型，不考虑数据是怎么生成的，只关心数据之间的差别。生成模型先计算联合概率分布，再通过贝叶斯公式转化为条件概率。
- 不同点：
1. 损失函数不同，SVM为合页损失函数，LR为交叉熵损失函数。两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。
2. 影响SVM决策面的只有少数的支持向量，需要做归一化处理。影响LR决策面的是所有样本点，决策面倾向于远离样本点比较多的类，若类不平衡，需先做平衡处理。
3. SVM是结构风险最小化，在训练误差和模型复杂度之间寻求平衡，防止过拟合，减小泛化误差，因为损失函数中带有L2正则项。LR是经验风险最小化，只考虑误差最小
4. 在解决非线性问题，SVM可以采用核函数机制，LR不能。因为SVM只有少数样本参与核计算，LR每个样本都要参与核计算，计算复杂度高。
5. LR可以给出每个点属于每一类的概率，SVM不能。
6. SVM适用于小数据集，且效果比LR好。LR计算简单，更适合大数据集，可以在线训练。
- 场景选择
1. 特征数量较大，和样本数差不多，选用LR或者带线性核的SVM。
2. 特征数量较小，样本数量一般，选用带高斯核函数的SVM。
3. 特征数量较小，样本很多，手动添加特征变成第一种情况。
- SVM核函数选择
1. 如果特征维数很高，往往线性可分（SVM解决非线性分类问题的思路就是将样本映射到更高维的特征空间中），可以采用LR或者线性核的SVM；
2. 如果样本数量很多，由于求解最优化问题的时候，目标函数涉及两两样本计算内积，使用高斯核明显计算量会大于线性核，所以手动添加一些特征，使得线性可分，然后可以用LR或者线性核的SVM；
3. 如果不满足上述两点，即特征维数少，样本数量正常，可以使用高斯核的SVM。

## KNN
- 基本概念
给定训练集，对于新样本在训练集中找到与该样本最近的K个样本，其中多数属于一类，那么该样本属于这一类。
### K值选取，距离度量，归一化必要性
- K值选取
1. K值较小，模型复杂，易学习到噪声。
2. K值较大，模型简单，忽略有用信息。
3. 一般先选取较小的K值，通过交叉验证选取最优K值。
- 距离度量

假设两个样本x1，x2分别有n个特征，Lp(x1, x2) = (∑|x1i - x2i| ** p) ** 1/p
1. p = 2 为欧氏距离
2. p = 1 为曼哈顿距离
3. p = 正无穷，切比雪夫距离 L(x1, x2) = max|x1i - x2i|为各个特征距离的最大值
- 归一化
1. 保证每个特征的同等重要性。

## Kmeans
- 定义
1. Kmeans是一种无监督的聚类算法，对于给定数据集，按样本间的距离划分为k个簇，使得簇内距离尽可能小，簇间距离尽可能大。
- k值选取
1. 根据经验选择合适k值，若没有先验知识，通过交叉验证选取。
- 步骤：假设簇数为k，最大迭代次数为N
1. 从数据集中随机选取k个样本作为初始的k个质心。（最好不能太近）
2. 对于n = 1, 2,,,n
   1. 计算各样本到质心的距离，与哪个质心距离最小，属于哪个簇
   2. 对于每个簇，重新计算质心
   3. 若所有k个质心向量都没有变化，输出簇划分
- 优点
1. 算法简单，实现容易，收敛速度快
2. 聚类效果好
3. 可解释性强
4. 主要需要调节的参数只有k
- 缺点
1. k值选取不好把握
2. 非凸数据集较难收敛
3. 采用迭代方法只能得到局部最优
4. 数据不平衡时分类效果不佳
5. 对噪声点较敏感
- https://www.cnblogs.com/pinard/p/6164214.html